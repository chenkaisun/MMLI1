\begin{thebibliography}{47}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ando and Zhang(2005)}]{Ando2005}
Rie~Kubota Ando and Tong Zhang. 2005.
\newblock A framework for learning predictive structures from multiple tasks
  and unlabeled data.
\newblock \emph{Journal of Machine Learning Research}, 6:1817--1853.

\bibitem[{Andrew and Gao(2007)}]{andrew2007scalable}
Galen Andrew and Jianfeng Gao. 2007.
\newblock Scalable training of {L1}-regularized log-linear models.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, pages 33--40.

\bibitem[{Beltagy et~al.(2019)Beltagy, Lo, and Cohan}]{scibert}
Iz~Beltagy, Kyle Lo, and Arman Cohan. 2019.
\newblock Scibert: A pretrained language model for scientific text.
\newblock \emph{arXiv preprint arXiv:1903.10676}.

\bibitem[{B{\"o}rschinger and
  Johnson(2011)}]{borschinger-johnson-2011-particle}
Benjamin B{\"o}rschinger and Mark Johnson. 2011.
\newblock \href {https://www.aclweb.org/anthology/U11-1004} {A particle filter
  algorithm for {B}ayesian wordsegmentation}.
\newblock In \emph{Proceedings of the Australasian Language Technology
  Association Workshop 2011}, pages 10--18, Canberra, Australia.

\bibitem[{Cho and Lee(2019)}]{biomedpp4}
Hyejin Cho and Hyunju Lee. 2019.
\newblock Biomedical named entity recognition using deep neural networks with
  contextual information.
\newblock \emph{BMC bioinformatics}, 20(1):1--11.

\bibitem[{Choi et~al.(2018)Choi, Levy, Choi, and Zettlemoyer}]{ultrafet}
Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018.
\newblock Ultra-fine entity typing.
\newblock \emph{arXiv preprint arXiv:1807.04905}.

\bibitem[{Dai et~al.(2019)Dai, Du, Li, and Song}]{fet_el}
Hongliang Dai, Donghong Du, Xin Li, and Yangqiu Song. 2019.
\newblock Improving fine-grained entity typing with entity linking.
\newblock \emph{arXiv preprint arXiv:1909.12079}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Diao et~al.(2021)Diao, Zhang, Ma, and Lu}]{sim_filter}
Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. 2021.
\newblock Similarity reasoning and filtration for image-text matching.
\newblock \emph{arXiv preprint arXiv:2101.01368}.

\bibitem[{Goodman et~al.(2016)Goodman, Vlachos, and
  Naradowsky}]{goodman-etal-2016-noise}
James Goodman, Andreas Vlachos, and Jason Naradowsky. 2016.
\newblock \href {https://doi.org/10.18653/v1/P16-1001} {Noise reduction and
  targeted exploration in imitation learning for {A}bstract {M}eaning
  {R}epresentation parsing}.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1--11, Berlin,
  Germany. Association for Computational Linguistics.

\bibitem[{Gusfield(1997)}]{Gusfield:97}
Dan Gusfield. 1997.
\newblock \emph{Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\bibitem[{Harper(2014)}]{harper-2014-learning}
Mary Harper. 2014.
\newblock \href {https://www.aclweb.org/anthology/C14-1001} {Learning from 26
  languages: Program management and science in the babel program}.
\newblock In \emph{Proceedings of {COLING} 2014, the 25th International
  Conference on Computational Linguistics: Technical Papers}, page~1, Dublin,
  Ireland. Dublin City University and Association for Computational
  Linguistics.

\bibitem[{He et~al.(2020)He, Wu, Yin, and Cai}]{kg4ner}
Qizhen He, Liang Wu, Yida Yin, and Heming Cai. 2020.
\newblock Knowledge-graph augmented word representations for named entity
  recognition.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 7919--7926.

\bibitem[{Jessop et~al.(2011)Jessop, Adams, Willighagen, Hawizy, and
  Murray-Rust}]{oscar4}
David~M Jessop, Sam~E Adams, Egon~L Willighagen, Lezan Hawizy, and Peter
  Murray-Rust. 2011.
\newblock Oscar4: a flexible architecture for chemical text-mining.
\newblock \emph{Journal of cheminformatics}, 3(1):1--12.

\bibitem[{Jin et~al.(2019)Jin, Hou, Li, and Dong}]{hierarchical_gcn}
Hailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong. 2019.
\newblock Fine-grained entity typing via hierarchical multi graph convolutional
  networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4970--4979.

\bibitem[{Lee et~al.(2020)Lee, Yoon, Kim, Kim, Kim, So, and Kang}]{biobert}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang. 2020.
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock \emph{Bioinformatics}, 36(4):1234--1240.

\bibitem[{Lee et~al.(2017)Lee, He, Lewis, and Zettlemoyer}]{e2ecoref}
Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017.
\newblock End-to-end neural coreference resolution.
\newblock \emph{arXiv preprint arXiv:1707.07045}.

\bibitem[{Li et~al.(2019{\natexlab{a}})Li, Huang, Ji, and Han}]{biomedpp2}
Diya Li, Lifu Huang, Heng Ji, and Jiawei Han. 2019{\natexlab{a}}.
\newblock Biomedical event extraction based on knowledge-driven tree-lstm.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 1421--1430.

\bibitem[{Li et~al.(2017)Li, Zhang, Fu, and Ji}]{biomedpp3}
Fei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. 2017.
\newblock A neural joint model for entity and relation extraction from
  biomedical text.
\newblock \emph{BMC bioinformatics}, 18(1):1--11.

\bibitem[{Li et~al.(2019{\natexlab{b}})Li, Yatskar, Yin, Hsieh, and
  Chang}]{visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
  2019{\natexlab{b}}.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock \emph{arXiv preprint arXiv:1908.03557}.

\bibitem[{Li et~al.(2020)Li, Zareian, Zeng, Whitehead, Lu, Ji, and
  Chang}]{LiACL2020}
Manling Li, Alireza Zareian, Qi~Zeng, Spencer Whitehead, Di~Lu, Heng Ji, and
  Shih-Fu Chang. 2020.
\newblock Cross-media structured common space for multimedia event extraction.
\newblock In \emph{Proc. The 58th Annual Meeting of the Association for
  Computational Linguistics (ACL2020)}.

\bibitem[{Lin and Ji(2019)}]{lin2019attentive}
Ying Lin and Heng Ji. 2019.
\newblock An attentive fine-grained entity typing model with latent type
  representation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6198--6203.

\bibitem[{Liu et~al.(2016)Liu, Chen, Jagannatha, and Yu}]{biomedie}
Feifan Liu, Jinying Chen, Abhyuday Jagannatha, and Hong Yu. 2016.
\newblock Learning for biomedical information extraction: Methodological review
  of recent advances.
\newblock \emph{arXiv preprint arXiv:1606.07993}.

\bibitem[{Liu et~al.(2018)Liu, Shen, Komandur~Elayavilli, Wang,
  Rastegar-Mojarad, Chaudhary, and Liu}]{biomedpp5}
Sijia Liu, Feichen Shen, Ravikumar Komandur~Elayavilli, Yanshan Wang, Majid
  Rastegar-Mojarad, Vipin Chaudhary, and Hongfang Liu. 2018.
\newblock Extracting chemical--protein relations using attention-based neural
  networks.
\newblock \emph{Database}, 2018.

\bibitem[{Liu et~al.(2020)Liu, Zhou, Zhao, Wang, Ju, Deng, and Wang}]{kbert}
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi~Ju, Haotang Deng, and Ping
  Wang. 2020.
\newblock K-bert: Enabling language representation with knowledge graph.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 2901--2908.

\bibitem[{L{\'o}pez et~al.(2019)L{\'o}pez, Heinzerling, and
  Strube}]{hyperbolic}
Federico L{\'o}pez, Benjamin Heinzerling, and Michael Strube. 2019.
\newblock Fine-grained entity typing in hyperbolic space.
\newblock \emph{arXiv preprint arXiv:1906.02505}.

\bibitem[{Nam et~al.(2017)Nam, Ha, and Kim}]{danmm}
Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017.
\newblock Dual attention networks for multimodal reasoning and matching.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 299--307.

\bibitem[{Nguyen et~al.(2020)Nguyen, Zhai, Yoshikawa, Fang, Druckenbrodt,
  Thorne, Hoessel, Akhondi, Cohn, Baldwin et~al.}]{chemu}
Dat~Quoc Nguyen, Zenan Zhai, Hiyori Yoshikawa, Biaoyan Fang, Christian
  Druckenbrodt, Camilo Thorne, Ralph Hoessel, Saber~A Akhondi, Trevor Cohn,
  Timothy Baldwin, et~al. 2020.
\newblock Chemu: named entity recognition and event extraction of chemical
  reactions from patents.
\newblock In \emph{European Conference on Information Retrieval}, pages
  572--579. Springer.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga et~al.}]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
  2019.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{arXiv preprint arXiv:1912.01703}.

\bibitem[{Peters et~al.(2019)Peters, Neumann, Logan~IV, Schwartz, Joshi, Singh,
  and Smith}]{knowbert}
Matthew~E Peters, Mark Neumann, Robert~L Logan~IV, Roy Schwartz, Vidur Joshi,
  Sameer Singh, and Noah~A Smith. 2019.
\newblock Knowledge enhanced contextual word representations.
\newblock \emph{arXiv preprint arXiv:1909.04164}.

\bibitem[{Poon and Vanderwende(2010)}]{biomedpp1}
Hoifung Poon and Lucy Vanderwende. 2010.
\newblock Joint inference for knowledge extraction from biomedical literature.
\newblock In \emph{Human language technologies: the 2010 annual conference of
  the North American chapter of the association for computational linguistics},
  pages 813--821.

\bibitem[{Qin et~al.(2020)Qin, Lin, Takanobu, Liu, Li, Ji, Huang, Sun, and
  Zhou}]{erica}
Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie
  Huang, Maosong Sun, and Jie Zhou. 2020.
\newblock Erica: Improving entity and relation understanding for pre-trained
  language models via contrastive learning.
\newblock \emph{arXiv preprint arXiv:2012.15022}.

\bibitem[{Rasooli and Tetreault(2015)}]{rasooli-tetrault-2015}
Mohammad~Sadegh Rasooli and Joel~R. Tetreault. 2015.
\newblock \href {http://arxiv.org/abs/1503.06733} {Yara parser: {A} fast and
  accurate dependency parser}.
\newblock \emph{Computing Research Repository}, arXiv:1503.06733.
\newblock Version 2.

\bibitem[{Stenetorp et~al.(2012)Stenetorp, Pyysalo, Topi{\'c}, Ohta, Ananiadou,
  and Tsujii}]{brat}
Pontus Stenetorp, Sampo Pyysalo, Goran Topi{\'c}, Tomoko Ohta, Sophia
  Ananiadou, and Junâ€™ichi Tsujii. 2012.
\newblock Brat: a web-based tool for nlp-assisted text annotation.
\newblock In \emph{Proceedings of the Demonstrations at the 13th Conference of
  the European Chapter of the Association for Computational Linguistics}, pages
  102--107.

\bibitem[{Su et~al.(2019)Su, Zhu, Cao, Li, Lu, Wei, and Dai}]{vlbert}
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
  2019.
\newblock Vl-bert: Pre-training of generic visual-linguistic representations.
\newblock \emph{arXiv preprint arXiv:1908.08530}.

\bibitem[{Tian et~al.(2020)Tian, Shen, Song, Xia, He, and Li}]{biomedpp6}
Yuanhe Tian, Wang Shen, Yan Song, Fei Xia, Min He, and Kenli Li. 2020.
\newblock Improving biomedical named entity recognition with syntactic
  information.
\newblock \emph{BMC bioinformatics}, 21(1):1--17.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}.

\bibitem[{Wang et~al.(2020)Wang, Tang, Duan, Wei, Huang, Cao, Jiang, Zhou
  et~al.}]{kadapter}
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao,
  Daxin Jiang, Ming Zhou, et~al. 2020.
\newblock K-adapter: Infusing knowledge into pre-trained models with adapters.
\newblock \emph{arXiv preprint arXiv:2002.01808}.

\bibitem[{Wang et~al.(2021)Wang, Gao, Zhu, Zhang, Liu, Li, and Tang}]{kepler}
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi
  Li, and Jian Tang. 2021.
\newblock Kepler: A unified model for knowledge embedding and pre-trained
  language representation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:176--194.

\bibitem[{Wei et~al.(2020)Wei, Zhang, Li, Zhang, and Wu}]{wei2020multi}
Xi~Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu. 2020.
\newblock Multi-modality cross attention network for image and sentence
  matching.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10941--10950.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz et~al.}]{huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al. 2019.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}.

\bibitem[{Xiong et~al.(2019)Xiong, Wu, Lei, Yu, Chang, Guo, and
  Wang}]{label_bias}
Wenhan Xiong, Jiawei Wu, Deren Lei, Mo~Yu, Shiyu Chang, Xiaoxiao Guo, and
  William~Yang Wang. 2019.
\newblock Imposing label-relational inductive bias for extremely fine-grained
  entity typing.
\newblock \emph{arXiv preprint arXiv:1903.02591}.

\bibitem[{Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka}]{gin}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}.

\bibitem[{Xu et~al.(2021)Xu, Li, Yuan, Wang, Wu, He, Liu, and Zhou}]{kplug}
Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Ying Liu,
  and Bowen Zhou. 2021.
\newblock K-plug: Knowledge-injected pre-trained language model for natural
  language understanding and generation in e-commerce.
\newblock \emph{arXiv preprint arXiv:2104.06960}.

\bibitem[{Yang and Mitchell(2019)}]{kblstm}
Bishan Yang and Tom Mitchell. 2019.
\newblock Leveraging knowledge bases in lstms for improving machine reading.
\newblock \emph{arXiv preprint arXiv:1902.09091}.

\bibitem[{Ye et~al.(2019)Ye, Rochan, Liu, and Wang}]{ye2019cross}
Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. 2019.
\newblock Cross-modal self-attention network for referring image segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10502--10511.

\bibitem[{Zhang et~al.(2019)Zhang, Han, Liu, Jiang, Sun, and Liu}]{ernie}
Zhengyan Zhang, Xu~Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019.
\newblock Ernie: Enhanced language representation with informative entities.
\newblock \emph{arXiv preprint arXiv:1905.07129}.

\end{thebibliography}
