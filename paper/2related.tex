

\section{Related Work}
% The followings are relevant works for now.

% \subsection{Dataset Construct}
% \subsection{Scientific IE}

% \textbf{\textcolor{purple}{In progress}}




\subsection{Fine-Grained Entity Typing}
\bluetext{There has been a wave of Fine-Grained Entity Typing (FET) methods in recent years~\cite{ultrafet, label_bias, fet_el, lin2019attentive,hierarchical_gcn, hyperbolic}. \citet{label_bias} captures label correlation by employing graph convolution network on label co-occurrence matrix. \citet{fet_el} make %, on the other hand, made 
use of an existing entity linker to obtain noisy external data in order to enrich and disambiguate mention representation. The author also use entity linking scores as additional features. \citet{lin2019attentive} exploits type inter-dependency with latent type representation. Previous FET methods, however, only focused on news domain where text comes from news or wikipedia article and speech.}  \heng{emphasize all of the previous work only focus on general news domain or wikipedia data}

Chemistry text, however, is largely different from news in that it's not only heavy in domain-specific knowledge, but also often has complex mention names not following morphological structure (sometimes alphanumeric encoding). Up until now, there has been no dataset or work in chemical FET, which is important for mining compound entities from unstructured chemistry literature. We have not only built a new benchmark, \bluetext{but also developed an effective FET framework that incorporates external structure and text knowledge.} \heng{not sure what you mean by 'external structural'?} \cheng{It seems that ``complex mention names" is a main new technical challenge. It would be great to amplify this message throughout the paper including a discussion of why the proposed model/architecture/framework can be expected to address this challenge and some empirical results to show the effectiveness in addressing the challenge (e.g., an example where previous methods failed to work because of the complex mention names but the proposed new method worked better. Another way to discuss the novelty here is to say the previous methods have NOT fully exploited the opportunity in our problem domain (e.g., external information/knowledge) and we propose a FET framework to enable full exploitation of external resources. }

% which is specific to chemistry domain.


\subsection{Multimodal Representation}
\heng{I deleted the following paragraph because it's too high-level and verbose.}
%In many domains, text are often accompanied with images or speeches, which are equally important perception units to text for humans to understand concepts. Mimicking how human process information, recent machine learning methods are also equipped with multimodal alignment, which aims to enrich the information representation after aligning concepts from different modalities (e.g., dog mention in text and dog object in image) and perform reasoning on multiple perceptions. This line of methods 

Multi-modal knowledge representation methods have been widely applied to tasks such as visual question answering and cross-modal retrieval between image and text. One line of deep-learning based alignment methods~\cite{sim_filter, wei2020multi,ye2019cross, danmm, LiACL2020} involves cross-modal alignment between separately learned word and image region representation. A recent popular line of research, including VisualBERT~\cite{visualbert} and VL-BERT\cite{vlbert}, integrates the reasoning process into pretraining, inspired from \cite{bert}. These models are fed with image-caption pairs and proceed to align regions and phrases by attention mechanism. 

Different from alignment among image, text, and audio, our method involves alignment between structures and description text, which is a phenomenon specific to chemistry and has hardly been explored in previous work. \\ \cheng{here it sounds like we are exploring a different kind of alignment which has not been studied before. "hardly" is vague; try to make it more specific. E.g., can we confidently say that it has NEVER been explored? or perhaps it has been explored, but we do it in a BETTER way (be specific in terms of where it's better)?} 

\subsection{Knowledge-Enhanced Language Representation}

%Like humans, deep learning models sometimes have difficult time understanding textual content without sufficient knowledge about concepts present in the sentences.
\heng{a lot more related work needs to be added in here. Check the related work in https://arxiv.org/pdf/2012.15022.pdf}
\bluetext{Recently, there has been a lot of work~\cite{knowbert, ernie, erica, kg4ner, kbert,kblstm,kadapter,kepler,kplug} on incorporating external knowledge into language understanding. In~\cite{kbert}, triples are injected into the sentences as domain knowledge and attach to the tokens in the sentence. \cite{kbert}, on the other hand, embeds words with KB concepts in an LSTM framework.}
% \heng{I deleted Tuan's paper because it's not published yet}
%In biomedical domain, \cite{} aligns nodes in span graph and knowledge graph to learn a more distinctive concept embedding for joint biomedical entity and relation extraction 

% \noindent Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference by Tuan: Use span enumeration from DYGIE~\cite{dygie} as a basis for jointly inferring entity existence, entity types and relation types. Also incorporate external linker to fetch candidates for each span, and perform graph conv to learn repr, finally classify\\

% \noindent K-BERT: Enabling Language Representation with Knowledge Graph: triples are injected into the sentences as domain knowledge and attach to the tokens in the sentence\\


% \noindent ERNIE~\cite{ernie} designed an architecture that fused knowledge graph 
% embedding into the language representation and added masking entity-token 
% alignment(named denoising entity autoencoder in the paper) as an 
% objective,besides MLM and NSP.\\
As a unique contribution, our work is the first to draw a line between local context and external (molecular) structural information\\ \cheng{is there a potential here to make this idea even more general? Can we say the general idea is to leverage non-textual external resources/information? }



% \subsection{Misc.}
% DYGIE~\cite{dygie}: Unlike previous BIO-based entity recognition systems (Collobert and Weston, 2008; Lample et al., 2016; Ma and Hovy, 2016) that assign a text span to at most one entity, this framework enumerates and represents all possible spans to recognize arbitrarily overlapping entities\\

% \noindent Reliability-aware Dynamic Feature Composition for Name Tagging: account for rareness of words by balancing between global and contextual repr\\
