

\section{Method}
\label{sec:method}
% \cheng{Before describing the method, it's better to briefly motivate the method. Perhaps remind the readers what are the key (new) challenges we need to address, and then briefly describe the proposed ideas for addressing those challenges informally. This would help readers see the big picture (in terms of novelty and justification) before going over the detail of the model.}
\bluetext{Chemistry literature is unique in that the mentions are often expressed in complex, unnatural forms, as shown in the snippet in Figure~\ref{fig:framework}. At the same time, external databases contains multimedia definition about chemical entity, such as chemical structure and natural language description. It is thus a natural idea to incorporate these different forms of representation of an entity to enhance the system's understanding on a chemicals. In our methodology, we develop an effective deep learning based model to implement such idea.}

The overall model architecture is presented in Figure~\ref{fig:framework}. Given a sentence $S$ marked with mentions, we first extract external information (chemical structure and natural language description) by linking to PubChem, one of the mostly used chemical database; we use its search API to fetch entity information given a mention name. We also used a modified version of $S$ that mask the entire mention name, to combat the issue of complex chemical name (\ref{sec:context}). We then proceed to extract features from local context and from external multimodal definition. The multimodal features are passed through cross-attention stage (\ref{sec:cmsa}) to learn a unified representation. Finally we use all features learned to classify the mention.

%for instance, graph neural network for structure input and Pretrained Language Model for 



\subsection{Original Text Embedding}
\label{sec:context}

Given the original sentence $S$, we first insert a marker symbol ``*'' at the start and end of the mention $m$ during preprocessing, following~\cite{atlop}; similar approach is also used in \cite{marker1, marker2}. The model first encodes the original sentence with SciBERT~\cite{scibert}, a Transformer~\cite{transformer} based language model pre-trained on biomedical text. Let $T=[t_1, t_2, ..., t_z]$ be the tokens in $S$ after tokenization (we implicitly assume the presence of [CLS] and [SEP] tokens and omit them from for cleanness), where $z$ is the number of tokens. Then we pass the tokens into SciBERT to to obtain contextual representations:
\begin{equation}
[\mathbf{t}_1, \mathbf{t}_2, ..., \mathbf{t}_z]=\text{SciBERT}([t_1, t_2, ..., t_z])\end{equation}

\noindent Where $\mathbf{T}=[\mathbf{t}_1, \mathbf{t}_2, ..., \mathbf{t}_z] \in \mathbb{R}^d$ and $d$ is the
number of hidden dimensions. We then use the embedding of ``*'' before $m$ as the mention embedding. Let us denote the mention embedding as $\mathbf{m}$.

% Then we follow \cite{e2ecoref} and compute the representation for mention $m$ by $$\mathbf{m}=\text{FFNN}_t([\mathbf{t}_{\text{START}(m)}, \mathbf{t}_{\text{END}(m)}, \hat t, \phi{(m)}])$$, where $\text{FFNN}_t$ is a feed forward neural network. $\text{END}(m)$ and $\text{START}(m)$ denote start and end indices for $m$. $\hat t$ is the representation based on attention to each token in $m$. 

%The $\text{FFNN}$ below will be the same.

\subsubsection{Context-only Embedding}
Since chemical entities often involve complex names that are difficult to be understood, we also produce a representation that rely less on the word structure of the mention, since the mention often not follows morphology (e.g., [3H]MK-801, NSC-406186, 8-azido-[alpha-32P]ATP). We first replace the entire span of mention by [MASK], then the modified sentence is embedded by SciBERT and the embedding for the [MASK] token is used as the corresponding context-only embedding for the mention, denoted $\mathbf{m}_\text{MASK}$. The context-focused embedding is then concatenated with mention embedding to represent local information, denoted by $\mathbf{m}_\text{L}=[\mathbf{m};\mathbf{m}_\text{MASK}]$.

\subsection{Multimodal Encoder}
\label{sec:cmsa}

% \chenkai{please ignore, whole thing will be changed}

As one of our core contributions, we propose to incorporate multimodal definition to expand chemical representation, and to combat the difficulty of understanding complex chemical mention name (e.g., (E)-3-(3,4-dihydroxyphenyl)prop-2-enoic acid) purely based on context and morphological structure. 

Specifically, we use API provided by PubChem as the entity linker to retrieve chemical structure and natural language description for each chemical mention. Chemical structure refers to a graph where bonds are edges and atoms are nodes, and description text discusses chemical properties (e.g., Aspirin is an orally administered non-steroidal antiinflammatory agent). 

% One another benefit is that system can implicitly learn to better cluster molecules even if a chemical entity is missing some modalities (e.g., only have structure available), since we map both modalities into the same embedding space.
To learn concepts from multiple modalities that better correlate with target label and to build more accurate representation of a molecule, we made use of the recently successful attention mechanism to co-embed concepts (or molecule property) in text and substructure in chemical graph in order to capture interaction between different modalities. 

Formally, let $G=(V,E)$ denote the chemical graph with $a$ nodes, and $D=[ d_1,d_2,...,d_b]$ denote the sequence of $b$ tokens after tokenizing description sentences. Similar to the embedding the sentence from literature, we embed the tokens with SciBERT  for which output is $\mathbf{D}=[\mathbf{d}_1,\mathbf{d}_2,...,\mathbf{d}_b]$. We also embed the nodes in chemical structure using Graph Isomorphism Network (with edge features)~\cite{gin}, a powerful graph neural network that can well capture different graph patterns. We randomly initialize embedding for each atom and bond type and use them to initialize node and edge embedding, and update node embeddings as below

\begin{equation}
\mathbf{n}_i^{l+1}=\text{FFNN}^{l+1}\bigg((1+\epsilon)\mathbf{n}_i^{l}+\sum_{j\in\mathcal{N}(i)}\mathbf{n}_j^{l}+\mathbf{e}^{l}_{j,i}\bigg)
\end{equation}
% _\theta^{(l+1)}
% \begin{equation}
% \begin{split}
% F = \{F_{x} \in  F_{c} &: (|S| > |C|) \\
%  &\quad \cap (\text{minPixels}  < |S| < \text{maxPixels}) \\
%  &\quad \cap (|S_{\text{conected}}| > |S| - \epsilon) \}
% \end{split}
% \end{equation}


\noindent where $\mathbf{n}_i^{l}\in \mathbb{R}^d$ is node representation for node $i$ at $l$-th layer, $\epsilon$ is a tuning hyperparameter, $\mathcal{N}(i)$ is the set of neighbours of node $i$, and FFNN is a feed forward neural network with two hidden layers (the first one maps from $\mathbb{R}^d$ to $\mathbb{R}^{2d}$ with Tanh activation, and the second one maps $\mathbb{R}^{2d}$ back to $\mathbb{R}^{d}$ without activation). We denote node representation $\mathbf{N}=[\mathbf{n}_1,\mathbf{n}_2,...,\mathbf{n}_a]$.


% \mathbf{n}_v^{(l+1)}=\text{MLP}_\theta^{(l+1)}\bigg((1+\epsilon)\mathbf{n}_v^{(l)}+\sum_{w\in\mathcal{N}(v)}\mathbf{n}_w^{(l)}+\mathbf{e}^{(l)}_{w,v}\bigg)


% d_{[CLS]}
We leverage self-attention mechanism of \cite{transformer} to learn dependency between different modalities. To achieve this, we first stack node and token embeddings as

\begin{equation}
\mathbf{X}=\begin{pmatrix}
\mathbf{N} \\
\mathbf{D}
\end{pmatrix}, \ \mathbf{X}\in\mathbb{R}^d
\end{equation}

\noindent where $\mathbf{X}\in\mathbb{R}^d$. Then the stacked embedding is passed through a Transformer layer to learn cross-modal association
\begin{equation}
[ \Tilde{\mathbf{n}}_1, \Tilde{\mathbf{n}}_2, ..., \Tilde{\mathbf{d}}_1, \Tilde{\mathbf{d}}_2...]=\text{Transformer}(\mathbf{X})\end{equation}

% Then we compute key values of the matrix by $\mathbf{Q}=\mathbf{X}\mathbf{W}^Q$,$\mathbf{K}=\mathbf{X}\mathbf{W}^K$, and $\mathbf{V}=\mathbf{X}\mathbf{W}^V$


% Then the attented representation is given by 

% $$\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{p_k}})\mathbf{V}$$

% where $\frac{1}{\sqrt{p_k}}$ is a scaling factor in \cite{transformer}. We used an average pooling to get multimodal representation $\mathbf{E}_{CM}$
\noindent and the cross-modal feature is then obtained by maxpooling the output from transformer layer

\begin{equation}
    \mathbf{f}_\text{cm}=\text{MaxPool}([ \Tilde{\mathbf{n}}_1, \Tilde{\mathbf{n}}_2, ..., \Tilde{\mathbf{d}}_1, \Tilde{\mathbf{d}}_2...])
\end{equation}


In addition, we preserve the unimodal graph representation by mean pooling over the node representation $\mathbf{N}$, to get $\mathbf{f}_{g}$. We also use the [CLS] token embedding $\mathbf{d}_{[CLS]}$ to represent unimodal text features. We then obtain a the multimodal definition vector

\begin{equation}
    \mathbf{f}=[\mathbf{f}_\text{cm};\mathbf{f}_{g};\mathbf{f}_{[CLS]}]
\end{equation}
\subsection{Final Prediction}

Let E denote the set of entity types. Lastly, we predict the final entity type by using features from both local context and multimodal information
\begin{equation}
\mathbf{p}=\text{Sigmoid}(\text{FFNN}([\mathbf{m}_\text{L};\mathbf{f}]))\end{equation}

\noindent where $\text{FFNN}$ is a feed forward neural network mapping from $\mathbb{R}^d$ to $\mathbb{R}^{|\text{E}|}$. $\mathbf{p}$ is the final probability distribution of classes.
% where $\mathbf{W}^f$ is a learnable weight matrix

\subsection{Training}
We use multi-label soft margin loss for training, that is,
\begin{equation}
\begin{split}
    \mathcal{L}=\frac{1}{C}\sum_{i=1}^{C}\Bigg( y_i\log\bigg(\frac{1}{1+e^{-x_i}}\bigg) +\\ (1-y_i)\log\bigg(\frac{e^{-x_i}}{1+e^{-x_i}}\bigg)\Bigg)
\end{split}
\end{equation}
\noindent In the equation, $C$ is number of classes, $y_i$ indicates true (binary) label for class $i$ and $x_i$ is the predicted probability for class $i$.



% as pointed out by~\cite{mmdl}, simple concatenate features from different modalities result in a shallow representation since the correlation, or interaction, between the modalities is highly non-linear.

%  it is important to
% learn joint embeddings to leverage the complementarity of
% multimodal data to represent such concepts more accurately

% to learn a more complete representation of the molucule, we need to ground the structure on textual propreties

% better
% features for one modality (e.g., video) can be
% learned if multiple modalities (e.g., audio and
% video) are present at feature learning time


% The flow of
% knowledge from one data modality to another is a challenging
% task.


% This allows for fast selection
% of a new trajectory by projecting a new environment/instruction pair and
% choosing its nearest-neighbor trajectory in this space.

% noise cancel
% so even only one part unreliable, can st infer , two similar compound similar label

% pick out important concepts that distinguish

% o perform reasoning with a variety of sensing modalities.

% unified repr

% Mapping to common space, so cancel noise
%  a shared embedding spac
% has the following advantages

% Although deep learning architectures do
% a good job of learning these vector representations in isolation, learning a single common representation across multiple
% modalities is a challenging task
% \begin{itemize}
%     \item It attends to important features in each modality, thus cancelling noise
%     \item Attention has proved its effectiveness in many visual and
% language tasks [23, 1, 7, 52, 50], it is designed to capture a
% better representation of image-sentence pairs based on their
% interactions.
% \end{itemize}




%After the above 
%Following~\cite{cmsa}, we also use self attention for encoding



% \subsection{Final Prediction}

% Concatenation, FFNN, Loss Function 